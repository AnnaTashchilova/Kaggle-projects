# Toxic Comment Classification Challenge
В данном исследовании был выполнен анализ комментариев из Википедии с целью выявления из них токсичных, то есть содержаших насилие, непристойные выражения, ненависть, угрозу и т.д.
Данные для исследования представлены на платформе Kaggle: 
https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge 
и содержат TRAIN выборку, TEST выборку и TEST LABELS, правильная классификация для теста, выложенная после окончания соревнования.
В соревновании предлагается выполнить multilabel классификацию комментариев, то есть один комментарий может принадлежать сразу нескольким классам токсичности:

*   Toxic – обычные токсичные
*   Severe_toxic – сильно токсичные
*   Obscene - непристойные
*   Threat - комментарии, содержащие насилие
*   Insult - комментарии, содержащие угрозу
*   Identity_hate - комментарии, содержащие ненависть к личности

## Preaprocessing
Preprocessing играет важную роль в дальнейшей успешной или неуспешной классификации текста. Поэтому для каждого комментария из обучающей и тестовой выборок были проведены следующие операции:

1.   Игнорируются все знаки препинания, цифры, слова, состоящии из одной или двух букв.
2.   Апостроф несет смысловую нагрузку только если находится в середине слова, тогда часть после апострофа убирается, так как не несет смысловой нагрузки.
3.   Далее используется лемматизатор, в который передается часть речи и слово приводится к начальной форме.
4.   На последнем этапе удаляются stop слова из словаря 'english'.

В результате получается список из слов, с которым будет удобно работать.
Пример препроцессинга комментария:
You, sir, are my hero. Any chance you remember what page that's on? ---> ['sir', 'hero', 'chance', 'remember', 'page']
